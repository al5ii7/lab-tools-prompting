{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "77f66dbe-192b-471c-9cb8-e9b365e61bbb",
   "metadata": {},
   "source": [
    "# Lab | Tools prompting\n",
    "\n",
    "**Replace the existing two tools decorators, by creating 3 new ones and adjust the prompts accordingly**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14b94240",
   "metadata": {},
   "source": [
    "### How to add ad-hoc tool calling capability to LLMs and Chat Models\n",
    "\n",
    ":::{.callout-caution}\n",
    "\n",
    "Some models have been fine-tuned for tool calling and provide a dedicated API for tool calling. Generally, such models are better at tool calling than non-fine-tuned models, and are recommended for use cases that require tool calling. Please see the [how to use a chat model to call tools](https://python.langchain.com/docs/how_to/tool_calling/) guide for more information.\n",
    "\n",
    "In this guide, we'll see how to add **ad-hoc** tool calling support to a chat model. This is an alternative method to invoke tools if you're using a model that does not natively support tool calling.\n",
    "\n",
    "We'll do this by simply writing a prompt that will get the model to invoke the appropriate tools. Here's a diagram of the logic:\n",
    "\n",
    "<br>\n",
    "\n",
    "![chain](https://education-team-2020.s3.eu-west-1.amazonaws.com/ai-eng/tool_chain.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0a22cb8-19e7-450a-9d1b-6848d2c81cd1",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "We'll need to install the following packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8c556c5e-b785-428b-8e7d-efd34a2a1adb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 25.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "%pip install --upgrade --quiet langchain langchain-community langchain_openai"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "897bc01e-cc2b-4400-8a64-db4aa56085d3",
   "metadata": {},
   "source": [
    "If you'd like to use LangSmith, uncomment the below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5efb4170-b95b-4d29-8f57-09509f3ba6df",
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "import os\n",
    "# os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "# os.environ[\"LANGCHAIN_API_KEY\"] = getpass.getpass()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ec6409b-21e5-4d0a-8a46-c4ef0b055dd3",
   "metadata": {},
   "source": [
    "You can select any of the given models for this how-to guide. Keep in mind that most of these models already [support native tool calling](https://python.langchain.com/docs/integrations/chat), so using the prompting strategy shown here doesn't make sense for these models, and instead you should follow the [how to use a chat model to call tools](https://python.langchain.com/docs/how_to/tool_calling/) guide.\n",
    "\n",
    "```{=mdx}\n",
    "import ChatModelTabs from \"@theme/ChatModelTabs\";\n",
    "\n",
    "<ChatModelTabs openaiParams={`model=\"gpt-4\"`} />\n",
    "```\n",
    "\n",
    "To illustrate the idea, we'll use `phi3` via Ollama, which does **NOT** have native support for tool calling. If you'd like to use `Ollama` as well follow [these instructions](https://python.langchain.com/docs/integrations/chat/ollama)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "424be968-2806-4d1a-a6aa-5499ae20fac5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aarya\\AppData\\Local\\Temp\\ipykernel_26036\\1427064109.py:3: LangChainDeprecationWarning: The class `Ollama` was deprecated in LangChain 0.3.1 and will be removed in 1.0.0. An updated version of the class exists in the :class:`~langchain-ollama package and should be used instead. To use it run `pip install -U :class:`~langchain-ollama` and import as `from :class:`~langchain_ollama import OllamaLLM``.\n",
      "  model = Ollama(model=\"phi3\")\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.llms import Ollama\n",
    "\n",
    "model = Ollama(model=\"phi3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a88a1463",
   "metadata": {},
   "source": [
    "\n",
    "#  How to Install and Run Ollama with the Phi-3 Model\n",
    "\n",
    "This guide walks you through installing **Ollama** and running the **Phi-3** model on Windows, macOS, and Linux.\n",
    "\n",
    "---\n",
    "\n",
    "## Windows\n",
    "\n",
    "1. **Download Ollama for Windows**  \n",
    "   Go to: [https://ollama.com/download](https://ollama.com/download)  \n",
    "   Download and run the installer.\n",
    "\n",
    "2. **Verify Installation**  \n",
    "   Open **Command Prompt** and type:\n",
    "   ```bash\n",
    "   ollama --version\n",
    "   ```\n",
    "\n",
    "3. **Run the Phi-3 Model**  \n",
    "   In the same terminal:\n",
    "   ```bash\n",
    "   ollama run phi3\n",
    "   ```\n",
    "\n",
    "4. **If you get a CUDA error (GPU memory issue)**  \n",
    "   Run Ollama in **CPU mode**:\n",
    "   ```bash\n",
    "   set OLLAMA_NO_CUDA=1\n",
    "   ollama run phi3\n",
    "   ```\n",
    "\n",
    "---\n",
    "\n",
    "##  macOS\n",
    "\n",
    "1. **Install via Homebrew**  \n",
    "   Open the Terminal and run:\n",
    "   ```bash\n",
    "   brew install ollama\n",
    "   ```\n",
    "\n",
    "2. **Run the Phi-3 Model**\n",
    "   ```bash\n",
    "   ollama run phi3\n",
    "   ```\n",
    "\n",
    "3. **To force CPU mode (no GPU)**\n",
    "   ```bash\n",
    "   export OLLAMA_NO_CUDA=1\n",
    "   ollama run phi3\n",
    "   ```\n",
    "\n",
    "---\n",
    "\n",
    "##  Linux\n",
    "\n",
    "1. **Install Ollama**  \n",
    "   Open a terminal and run:\n",
    "   ```bash\n",
    "   curl -fsSL https://ollama.com/install.sh | sh\n",
    "   ```\n",
    "\n",
    "2. **Run the Phi-3 Model**\n",
    "   ```bash\n",
    "   ollama run phi3\n",
    "   ```\n",
    "\n",
    "3. **To force CPU mode (no GPU)**\n",
    "   ```bash\n",
    "   export OLLAMA_NO_CUDA=1\n",
    "   ollama run phi3\n",
    "   ```\n",
    "\n",
    "---\n",
    "\n",
    "##  Notes\n",
    "\n",
    "- The first time you run `ollama run phi3`, it will **download the model**, so make sure you’re connected to the internet.\n",
    "- Once downloaded, it works **offline**.\n",
    "- Keep the terminal open and running in the background while using Ollama from your code or notebook.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68946881",
   "metadata": {},
   "source": [
    "## Create a tool\n",
    "\n",
    "First, let's create an `add` and `multiply` tools. For more information on creating custom tools, please see [this guide](https://python.langchain.com/docs/how_to/custom_tools/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4548e6fa-0f9b-4d7a-8fa5-66cec0350e5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--\n",
      "multiply\n",
      "Multiply two numbers together.\n",
      "{'x': {'title': 'X', 'type': 'number'}, 'y': {'title': 'Y', 'type': 'number'}}\n",
      "--\n",
      "add\n",
      "Add two numbers.\n",
      "{'x': {'title': 'X', 'type': 'integer'}, 'y': {'title': 'Y', 'type': 'integer'}}\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.tools import tool\n",
    "\n",
    "\n",
    "@tool\n",
    "def multiply(x: float, y: float) -> float:\n",
    "    \"\"\"Multiply two numbers together.\"\"\"\n",
    "    return x * y\n",
    "\n",
    "\n",
    "@tool\n",
    "def add(x: int, y: int) -> int:\n",
    "    \"Add two numbers.\"\n",
    "    return x + y\n",
    "\n",
    "\n",
    "tools = [multiply, add]\n",
    "\n",
    "# Let's inspect the tools\n",
    "for t in tools:\n",
    "    print(\"--\")\n",
    "    print(t.name)\n",
    "    print(t.description)\n",
    "    print(t.args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "be77e780",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20.0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multiply.invoke({\"x\": 4, \"y\": 5})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15dd690e-e54d-4209-91a4-181f69a452ac",
   "metadata": {},
   "source": [
    "## Creating our prompt\n",
    "\n",
    "We'll want to write a prompt that specifies the tools the model has access to, the arguments to those tools, and the desired output format of the model. In this case we'll instruct it to output a JSON blob of the form `{\"name\": \"...\", \"arguments\": {...}}`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2063b564-25ca-4729-a45f-ba4633175b04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "multiply(x: float, y: float) -> float - Multiply two numbers together.\n",
      "add(x: int, y: int) -> int - Add two numbers.\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.tools import render_text_description\n",
    "\n",
    "rendered_tools = render_text_description(tools)\n",
    "print(rendered_tools)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f02f1dce-76e7-4ca9-9bac-5af496131fe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = f\"\"\"\\\n",
    "You are an assistant that has access to the following set of tools. \n",
    "Here are the names and descriptions for each tool:\n",
    "\n",
    "{rendered_tools}\n",
    "\n",
    "Given the user input, return the name and input of the tool to use. \n",
    "Return your response as a JSON blob with 'name' and 'arguments' keys.\n",
    "\n",
    "The `arguments` should be a dictionary, with keys corresponding \n",
    "to the argument names and the values corresponding to the requested values.\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [(\"system\", system_prompt), (\"user\", \"{input}\")]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f8623e03-60eb-4439-b57b-ecbcebc61b58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```json\n",
      "{\n",
      "  \"name\": \"add\",\n",
      "  \"arguments\": {\n",
      "    \"x\": 3,\n",
      "    \"y\": 1132\n",
      "  }\n",
      "}\n",
      "```\n",
      "\n",
      "To get the result of this operation using our tools:\n",
      "\n",
      "```json\n",
      "{\n",
      "  \"name\": \"add\",\n",
      "  \"arguments\": {\n",
      "    \"x\": 3,\n",
      "    \"y\": 1132\n",
      "  }\n",
      "}\n",
      "```\n",
      "Since we are working with integers in this case, the `multiply` tool is not relevant. However, if it were necessary to use a different type of input or operation based on user needs beyond what'ged provided here (for example adding float numbers), then our JSON response could be modified accordingly:\n",
      "\n",
      "```json\n",
      "{\n",
      "  \"name\": \"add\",\n",
      "  \"arguments\": {\n",
      "    \"x\": 3.0, // Converted to a floating point number for the purpose of this hypothetical scenario where we might want floats instead of integers in our operation result or inputs.\n",
      "    \"y\": 1132.0\n",
      "  }\n",
      "}\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "chain = prompt | model\n",
    "message = chain.invoke({\"input\": \"what's 3 plus 1132\"})\n",
    "\n",
    "# Let's take a look at the output from the model\n",
    "# if the model is an LLM (not a chat model), the output will be a string.\n",
    "if isinstance(message, str):\n",
    "    print(message)\n",
    "else:  # Otherwise it's a chat model\n",
    "    print(message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14df2cd5-b6fa-4b10-892d-e8692c7931e5",
   "metadata": {},
   "source": [
    "## Adding an output parser\n",
    "\n",
    "We'll use the `JsonOutputParser` for parsing our models output to JSON."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f129f5bd-127c-4c95-8f34-8f437da7ca8f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': 'multiply', 'arguments': {'x': 13, 'y': 4}}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "\n",
    "chain = prompt | model | JsonOutputParser()\n",
    "chain.invoke({\"input\": \"what's thirteen times 4\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1f08255-f146-4f4a-be43-5c21c1d3ae83",
   "metadata": {},
   "source": [
    ":::{.callout-important}\n",
    "\n",
    "🎉 Amazing! 🎉 We now instructed our model on how to **request** that a tool be invoked.\n",
    "\n",
    "Now, let's create some logic to actually run the tool!\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e29dd4c-8eb5-457f-92d1-8add076404dc",
   "metadata": {},
   "source": [
    "## Invoking the tool 🏃\n",
    "\n",
    "Now that the model can request that a tool be invoked, we need to write a function that can actually invoke \n",
    "the tool.\n",
    "\n",
    "The function will select the appropriate tool by name, and pass to it the arguments chosen by the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "faee95e0-4095-4310-991f-9e9465c6738e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any, Dict, Optional, TypedDict\n",
    "\n",
    "from langchain_core.runnables import RunnableConfig\n",
    "\n",
    "\n",
    "class ToolCallRequest(TypedDict):\n",
    "    \"\"\"A typed dict that shows the inputs into the invoke_tool function.\"\"\"\n",
    "\n",
    "    name: str\n",
    "    arguments: Dict[str, Any]\n",
    "\n",
    "\n",
    "def invoke_tool(\n",
    "    tool_call_request: ToolCallRequest, config: Optional[RunnableConfig] = None\n",
    "):\n",
    "    \"\"\"A function that we can use the perform a tool invocation.\n",
    "\n",
    "    Args:\n",
    "        tool_call_request: a dict that contains the keys name and arguments.\n",
    "            The name must match the name of a tool that exists.\n",
    "            The arguments are the arguments to that tool.\n",
    "        config: This is configuration information that LangChain uses that contains\n",
    "            things like callbacks, metadata, etc.See LCEL documentation about RunnableConfig.\n",
    "\n",
    "    Returns:\n",
    "        output from the requested tool\n",
    "    \"\"\"\n",
    "    tool_name_to_tool = {tool.name: tool for tool in tools}\n",
    "    name = tool_call_request[\"name\"]\n",
    "    requested_tool = tool_name_to_tool[name]\n",
    "    return requested_tool.invoke(tool_call_request[\"arguments\"], config=config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4957532-9e0c-47f6-bb62-0fd789ac1d3e",
   "metadata": {},
   "source": [
    "Let's test this out 🧪!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d0ea3b2a-8fb2-4016-83c8-a5d3e78fedbc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15.0"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "invoke_tool({\"name\": \"multiply\", \"arguments\": {\"x\": 3, \"y\": 5}})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "715af6e1-935d-4bc0-a3d2-646ecf8a329b",
   "metadata": {},
   "source": [
    "## Let's put it together\n",
    "\n",
    "Let's put it together into a chain that creates a calculator with add and multiplication capabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0555b384-fde6-4404-86e0-7ea199003d58",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "53.83784653"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain = prompt | model | JsonOutputParser() | invoke_tool\n",
    "chain.invoke({\"input\": \"what's thirteen times 4.14137281\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4a9c5aa-f60a-4017-af6f-1ff6e04bfb61",
   "metadata": {},
   "source": [
    "## Returning tool inputs\n",
    "\n",
    "It can be helpful to return not only tool outputs but also tool inputs. We can easily do this with LCEL by `RunnablePassthrough.assign`-ing the tool output. This will take whatever the input is to the RunnablePassrthrough components (assumed to be a dictionary) and add a key to it while still passing through everything that's currently in the input:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "45404406-859d-4caa-8b9d-5838162c80a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': 'multiply',\n",
       " 'arguments': {'x': 13, 'y': 4.14137281},\n",
       " 'output': 53.83784653}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "chain = (\n",
    "    prompt | model | JsonOutputParser() | RunnablePassthrough.assign(output=invoke_tool)\n",
    ")\n",
    "chain.invoke({\"input\": \"what's thirteen times 4.14137281\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1797fe82-ea35-4cba-834a-1caf9740d184",
   "metadata": {},
   "source": [
    "## What's next?\n",
    "\n",
    "This how-to guide shows the \"happy path\" when the model correctly outputs all the required tool information.\n",
    "\n",
    "In reality, if you're using more complex tools, you will start encountering errors from the model, especially for models that have not been fine tuned for tool calling and for less capable models.\n",
    "\n",
    "You will need to be prepared to add strategies to improve the output from the model; e.g.,\n",
    "\n",
    "1. Provide few shot examples.\n",
    "2. Add error handling (e.g., catch the exception and feed it back to the LLM to ask it to correct its previous output)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "899d60d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Required Imports\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_community.llms import Ollama\n",
    "from langchain.chains import LLMChain\n",
    "\n",
    "#  Setup LLaMA 3 (make sure it's running with `ollama run llama3`)\n",
    "llm = Ollama(model=\"llama3\")\n",
    "\n",
    "#  Few-shot Prompt Template\n",
    "prompt = PromptTemplate.from_template(\"\"\"\n",
    "You are a helpful assistant. When asked a math question, respond with the correct tool call format.\n",
    "\n",
    "Examples:\n",
    "User: What is 2 plus 3?\n",
    "Assistant: Calling tool: add(x=2, y=3) → 5\n",
    "\n",
    "User: What is 4 plus 9?\n",
    "Assistant: Calling tool: add(x=4, y=9) → 13\n",
    "\n",
    "Now answer:\n",
    "User: What is {x} plus {y}?\n",
    "Assistant:\n",
    "\"\"\")\n",
    "\n",
    "#  Build the chain\n",
    "chain = LLMChain(llm=llm, prompt=prompt)\n",
    "\n",
    "#  Function to parse model output\n",
    "def parse_output(output):\n",
    "    if \"Calling tool:\" in output and \"add(\" in output:\n",
    "        return output\n",
    "    else:\n",
    "        raise ValueError(\"Invalid tool call format\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c2a8ccd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Function with error handling\n",
    "def ask_question(x, y):\n",
    "    try:\n",
    "        response = chain.invoke({\"x\": x, \"y\": y})[\"text\"]\n",
    "        return parse_output(response)\n",
    "    except Exception as e:\n",
    "        print(\"First try failed. Retrying with correction prompt...\")\n",
    "        correction_prompt = f\"The model gave this incorrect response: '{response}'. Please reformat it like: Calling tool: add(x={x}, y={y}) → result.\"\n",
    "        retry = llm.invoke(correction_prompt)\n",
    "        return retry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9102aac3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First try failed. Retrying with correction prompt...\n"
     ]
    },
    {
     "ename": "UnboundLocalError",
     "evalue": "cannot access local variable 'response' where it is not associated with a value",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOllamaEndpointNotFoundError\u001b[0m               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[27], line 4\u001b[0m, in \u001b[0;36mask_question\u001b[1;34m(x, y)\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m----> 4\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mchain\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mx\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43my\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parse_output(response)\n",
      "File \u001b[1;32mc:\\Users\\aarya\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\langchain\\chains\\base.py:170\u001b[0m, in \u001b[0;36mChain.invoke\u001b[1;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[0;32m    169\u001b[0m     run_manager\u001b[38;5;241m.\u001b[39mon_chain_error(e)\n\u001b[1;32m--> 170\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[0;32m    171\u001b[0m run_manager\u001b[38;5;241m.\u001b[39mon_chain_end(outputs)\n",
      "File \u001b[1;32mc:\\Users\\aarya\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\langchain\\chains\\base.py:160\u001b[0m, in \u001b[0;36mChain.invoke\u001b[1;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[0;32m    158\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_inputs(inputs)\n\u001b[0;32m    159\u001b[0m outputs \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m--> 160\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    161\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m new_arg_supported\n\u001b[0;32m    162\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(inputs)\n\u001b[0;32m    163\u001b[0m )\n\u001b[0;32m    165\u001b[0m final_outputs: Dict[\u001b[38;5;28mstr\u001b[39m, Any] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprep_outputs(\n\u001b[0;32m    166\u001b[0m     inputs, outputs, return_only_outputs\n\u001b[0;32m    167\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\aarya\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\langchain\\chains\\llm.py:126\u001b[0m, in \u001b[0;36mLLMChain._call\u001b[1;34m(self, inputs, run_manager)\u001b[0m\n\u001b[0;32m    121\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_call\u001b[39m(\n\u001b[0;32m    122\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    123\u001b[0m     inputs: Dict[\u001b[38;5;28mstr\u001b[39m, Any],\n\u001b[0;32m    124\u001b[0m     run_manager: Optional[CallbackManagerForChainRun] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    125\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Dict[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mstr\u001b[39m]:\n\u001b[1;32m--> 126\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    127\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcreate_outputs(response)[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\aarya\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\langchain\\chains\\llm.py:138\u001b[0m, in \u001b[0;36mLLMChain.generate\u001b[1;34m(self, input_list, run_manager)\u001b[0m\n\u001b[0;32m    137\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mllm, BaseLanguageModel):\n\u001b[1;32m--> 138\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mllm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_prompt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    139\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprompts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    140\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    141\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    142\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mllm_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    143\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    144\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\aarya\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\langchain_core\\language_models\\llms.py:764\u001b[0m, in \u001b[0;36mBaseLLM.generate_prompt\u001b[1;34m(self, prompts, stop, callbacks, **kwargs)\u001b[0m\n\u001b[0;32m    763\u001b[0m prompt_strings \u001b[38;5;241m=\u001b[39m [p\u001b[38;5;241m.\u001b[39mto_string() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m prompts]\n\u001b[1;32m--> 764\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt_strings\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\aarya\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\langchain_core\\language_models\\llms.py:971\u001b[0m, in \u001b[0;36mBaseLLM.generate\u001b[1;34m(self, prompts, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[0m\n\u001b[0;32m    957\u001b[0m     run_managers \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m    958\u001b[0m         callback_manager\u001b[38;5;241m.\u001b[39mon_llm_start(\n\u001b[0;32m    959\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_serialized,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    969\u001b[0m         )\n\u001b[0;32m    970\u001b[0m     ]\n\u001b[1;32m--> 971\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_generate_helper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    972\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprompts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    973\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    974\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    975\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnew_arg_supported\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mbool\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mnew_arg_supported\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    976\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    977\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    978\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(missing_prompts) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\aarya\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\langchain_core\\language_models\\llms.py:790\u001b[0m, in \u001b[0;36mBaseLLM._generate_helper\u001b[1;34m(self, prompts, stop, run_managers, new_arg_supported, **kwargs)\u001b[0m\n\u001b[0;32m    788\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    789\u001b[0m     output \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m--> 790\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_generate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    791\u001b[0m \u001b[43m            \u001b[49m\u001b[43mprompts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    792\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    793\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;66;43;03m# TODO: support multiple run managers\u001b[39;49;00m\n\u001b[0;32m    794\u001b[0m \u001b[43m            \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_managers\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    795\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    796\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    797\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m new_arg_supported\n\u001b[0;32m    798\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate(prompts, stop\u001b[38;5;241m=\u001b[39mstop)\n\u001b[0;32m    799\u001b[0m     )\n\u001b[0;32m    800\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32mc:\\Users\\aarya\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\langchain_community\\llms\\ollama.py:437\u001b[0m, in \u001b[0;36mOllama._generate\u001b[1;34m(self, prompts, stop, images, run_manager, **kwargs)\u001b[0m\n\u001b[0;32m    436\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m prompt \u001b[38;5;129;01min\u001b[39;00m prompts:\n\u001b[1;32m--> 437\u001b[0m     final_chunk \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_stream_with_aggregation\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    438\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    439\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    440\u001b[0m \u001b[43m        \u001b[49m\u001b[43mimages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mimages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    441\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    442\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    443\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    444\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    445\u001b[0m     generations\u001b[38;5;241m.\u001b[39mappend([final_chunk])\n",
      "File \u001b[1;32mc:\\Users\\aarya\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\langchain_community\\llms\\ollama.py:349\u001b[0m, in \u001b[0;36m_OllamaCommon._stream_with_aggregation\u001b[1;34m(self, prompt, stop, run_manager, verbose, **kwargs)\u001b[0m\n\u001b[0;32m    348\u001b[0m final_chunk: Optional[GenerationChunk] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 349\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstream_resp\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create_generate_stream\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m    350\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstream_resp\u001b[49m\u001b[43m:\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\aarya\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\langchain_community\\llms\\ollama.py:194\u001b[0m, in \u001b[0;36m_OllamaCommon._create_generate_stream\u001b[1;34m(self, prompt, stop, images, **kwargs)\u001b[0m\n\u001b[0;32m    193\u001b[0m payload \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprompt\u001b[39m\u001b[38;5;124m\"\u001b[39m: prompt, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimages\u001b[39m\u001b[38;5;124m\"\u001b[39m: images}\n\u001b[1;32m--> 194\u001b[0m \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create_stream\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    195\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpayload\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpayload\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    196\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    197\u001b[0m \u001b[43m    \u001b[49m\u001b[43mapi_url\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbase_url\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/api/generate\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    198\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    199\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\aarya\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\langchain_community\\llms\\ollama.py:266\u001b[0m, in \u001b[0;36m_OllamaCommon._create_stream\u001b[1;34m(self, api_url, payload, stop, **kwargs)\u001b[0m\n\u001b[0;32m    265\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m response\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m404\u001b[39m:\n\u001b[1;32m--> 266\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m OllamaEndpointNotFoundError(\n\u001b[0;32m    267\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOllama call failed with status code 404. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    268\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMaybe your model is not found \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    269\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mand you should pull the model with `ollama pull \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m`.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    270\u001b[0m     )\n\u001b[0;32m    271\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[1;31mOllamaEndpointNotFoundError\u001b[0m: Ollama call failed with status code 404. Maybe your model is not found and you should pull the model with `ollama pull llama3`.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[28], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# the question: What's 3 plus 1132?\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mask_question\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1132\u001b[39;49m\u001b[43m)\u001b[49m)\n",
      "Cell \u001b[1;32mIn[27], line 8\u001b[0m, in \u001b[0;36mask_question\u001b[1;34m(x, y)\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m      7\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFirst try failed. Retrying with correction prompt...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 8\u001b[0m     correction_prompt \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe model gave this incorrect response: \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[43mresponse\u001b[49m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m. Please reformat it like: Calling tool: add(x=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mx\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, y=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00my\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) → result.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      9\u001b[0m     retry \u001b[38;5;241m=\u001b[39m llm\u001b[38;5;241m.\u001b[39minvoke(correction_prompt)\n\u001b[0;32m     10\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m retry\n",
      "\u001b[1;31mUnboundLocalError\u001b[0m: cannot access local variable 'response' where it is not associated with a value"
     ]
    }
   ],
   "source": [
    "# the question: What's 3 plus 1132?\n",
    "print(ask_question(3, 1132))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
